![Alt Text](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-2-Banner.original.jpg)
# Fine-Tuning Gemma2 in Keras using LoRA
## Overview
This project contains an implementation of fine-tuning the Gemma language model using the Low-Rank Adaptation (LoRA) technique on the Alpaca dataset. The goal is to adapt the pre-trained Gemma model to better handle instruction-following tasks
## Dataset
The Alpaca dataset is a dataset created by Stanford researchers to fine-tune large language models (LLMs) for instruction-following tasks. It is based on OpenAI's text-davinci-003 model and contains 52,000 instruction-response pairs. The dataset was generated by taking a small set of 175 human-written prompts and using GPT-3.5 (text-davinci-003) to generate diverse instruction-response pairs automatically.
