![Alt Text](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-2-Banner.original.jpg)
# Fine-Tuning Gemma2 in Keras using LoRA
## Overview
This repository contains an implementation of fine-tuning the Gemma language model using the Low-Rank Adaptation (LoRA) technique on the Alpaca dataset. The goal is to adapt the pre-trained Gemma model to better handle instruction-following tasks. You can view and run the notebook through this [link](https://www.kaggle.com/code/beasttitan/gemma-fine-tuning).  
To use the Gemma LLM in this notebook you need to request access on Kaggle through this [link](https://www.kaggle.com/models/keras/gemma) 
## Dataset
The Alpaca dataset is a dataset created by Stanford researchers to fine-tune large language models (LLMs) for instruction-following tasks. It is based on OpenAI's text-davinci-003 model and contains 52,000 instruction-response pairs. The dataset was generated by taking a small set of 175 human-written prompts and using GPT-3.5 (text-davinci-003) to generate diverse instruction-response pairs automatically. To know more check this [link](https://crfm.stanford.edu/2023/03/13/alpaca.html)
